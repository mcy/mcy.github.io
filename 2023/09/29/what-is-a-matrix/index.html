<!DOCTYPE html> <html lang="en-us"> <head> <link href="https://gmpg.org/xfn/11" rel="profile"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta http-equiv="content-type" content="text/html; charset=utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1"> <title> What is a Matrix? A Miserable Pile of Coefficients! &middot; mcyoung </title> <link rel="stylesheet" href="https://mcyoung.xyz/public/css/syntax.css"> <link rel="stylesheet" href="https://mcyoung.xyz/public/css/syntax-overrides.css"> <link rel="stylesheet" href="https://mcyoung.xyz/public/css/style.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous"> <link rel="preload" href="https://mcyoung.xyz/public/fonts/abril-fatface.woff2" as="font" type="font/woff2" crossorigin> <link rel="preload" href="https://mcyoung.xyz/public/fonts/rokkitt.woff2" as="font" type="font/woff2" crossorigin> <link rel="preload" href="https://mcyoung.xyz/public/fonts/rokkitt-italic.woff2" as="font" type="font/woff2" crossorigin> <link rel="preload" href="https://mcyoung.xyz/public/fonts/spline-mono.woff2" as="font" type="font/woff2" crossorigin> <link rel="preload" href="https://mcyoung.xyz/public/fonts/spline-mono-italic.woff2" as="font" type="font/woff2" crossorigin> <link rel="shortcut icon" href="https://mcyoung.xyz/public/favicon.ico"> <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml"> <script src="https://mcyoung.xyz/public/js/minimap.js"></script> <script data-goatcounter="https://mcy.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script> <meta name="twitter:card" content="summary_large_image"> <meta name="twitter:title" content="What is a Matrix? A Miserable Pile of Coefficients! &middot; mcyoung"> <meta name="twitter:image" content="https://mcyoung.xyz/og/what-is-a-matrix-700b8363691e8c4f01b038d2f8dee86d5147ed9b.png"> <meta property="og:title" content="What is a Matrix? A Miserable Pile of Coefficients! &middot; mcyoung"> <meta property="og:type" content="object"> <meta property="og:image" content="https://mcyoung.xyz/og/what-is-a-matrix-700b8363691e8c4f01b038d2f8dee86d5147ed9b.png"> <meta property="og:height" content="630"> <meta property="og:width" content="1200"> <meta property="og:url" content="https://mcyoung.xyz/2023/09/29/what-is-a-matrix/"> </head> <body> <div class="sidebar"> <div class="sidebar-avatar hide-if-mobile"> <a href="https://mcyoung.xyz/posts"> <img class="sidebar-avatar" src="https://mcyoung.xyz/public/images/avatar.png" alt="Yeah, I drew this. Check out my art blog."></a> </div> <div class="container sidebar-sticky"> <div class="sidebar-about"> <h1><a href="https://mcyoung.xyz/posts"> mcyoung </a></h1> <div class="lead hide-if-mobile">I'm Miguel. I write about compilers, performance, and silly computer things. I also draw Pokémon. </div> </div> <hr class="hide-if-mobile"/> <nav class="sidebar-nav"> <a class="sidebar-nav-item" href="https://mcyoung.xyz">Home</a> • <a class="sidebar-nav-item" href="https://mcyoung.xyz/about">About</a> • <a class="sidebar-nav-item" href="https://mcyoung.xyz/posts">Posts</a> • <a class="sidebar-nav-item" href="https://mcyoung.xyz/tags">Tags</a> </nav> <nav class="sidebar-nav"> <a class="sidebar-nav-item " href="https://art.mcyoung.xyz">Art</a> • <a class="sidebar-nav-item" href="https://github.com/mcy">GitHub</a> • <a class="sidebar-nav-item" href="https://mcyoung.xyz/resume">Resumé</a> • <a class="sidebar-nav-item" href="https://mcyoung.xyz/syllabus">Syllabus</a> </nav> <br class="hide-if-mobile"/> <span class="hide-if-mobile"> <a href="https://creativecommons.org/licenses/by-sa/4.0/legalcode">CC BY-SA</a> • <a href="https://varz.mcyoung.xyz/">Site Analytics</a> <br> &copy; 2024 Miguel Young de la Sota</span> </div> </div> <div class="content container"><div class="post-title"> <span class="post-meta"> 2023-09-29 • 2009 words • 11 minutes <br class="show-if-mobile"/> <span class="hide-if-mobile">•</span> <a href="https://mcyoung.xyz/tags.html#math">#math</a> </span> <h1><a href="/2023/09/29/what-is-a-matrix/"> What is a Matrix? A Miserable Pile of Coefficients! </a></h1> </div> <div class="post"> <p>Linear algebra is undoubtedly the most useful field in all of algebra. It finds applications in all kinds of science and engineering, like quantum mechanics, graphics programming, and machine learning. It is the “most well-behaved” algebraic theory, in that other abstract algebra topics often try to approximate linear algebra, when possible.</p> <p>For many students, linear algebra means vectors and matrices and determinants, and complex formulas for computing them. Matrices, in particular, come equipped with a fairly complicated, and <em>a fortiori</em> convoluted, multiplication operation.</p> <p>This is not the only way to teach linear algebra, of course. Matrices and their multiplication appear complicated, but actually are a natural and compact way to represent a particular type of <em>function</em>, i.e., a linear map (or linear transformation).</p> <p>This article is a short introduction to viewing linear algebra from the perspective of abstract algebra, from which matrices arise as a computational tool, rather than an object of study in and of themselves. I do assume some degree of familiarity with the idea of a matrix.</p> <h2 id="linear-spaces"><a href="#linear-spaces">Linear Spaces</a></h2> <p>Most linear algebra courses open with a description of vectors in Euclidean space: <code class="language-plaintext highlighter-rouge">$\R^n$</code>. Vectors there are defined as tuples of real numbers that can be added, multiplied, and scaled. Two vectors can be combined into a number through the dot product. Vectors come equipped with a notion of magnitude and direction.</p> <p>However, this highly geometric picture can be counterproductive, since it is hard to apply geometric intuition directly to higher dimensions. It also obscures how this connects to working over a different number system, like the complex numbers.</p> <p>Instead, I’d like to open with the concept of a <em>linear space</em>, which is somewhat more abstract than a vector space<sup id="fnref:vs-in-generality" role="doc-noteref"><a href="#fn:vs-in-generality" class="footnote" rel="footnote">1</a></sup>.</p> <p>First, we will need a notion of a “coefficient”, which is essentially something that you can do arithmetic with. We will draw coefficients from a designated <em>ground field</em> <code class="language-plaintext highlighter-rouge">$K$</code>. A field is a setting for doing arithmetic: a set of objects that can be added, subtracted, and multiplied, and divided in the “usual fashion” along with special <code class="language-plaintext highlighter-rouge">$0$</code> and <code class="language-plaintext highlighter-rouge">$1$</code> values. E.g. <code class="language-plaintext highlighter-rouge">$a + 0 = a$</code>, <code class="language-plaintext highlighter-rouge">$1a = a$</code>, <code class="language-plaintext highlighter-rouge">$a(b + c) = ab + ac$</code>, and so on.</p> <p>Not only are the real numbers <code class="language-plaintext highlighter-rouge">$\R$</code> a field, but so are the complex numbers <code class="language-plaintext highlighter-rouge">$\C$</code>, and the rational numbers <code class="language-plaintext highlighter-rouge">$\Q$</code>. If we drop the “division” requirement, we can also include the integers <code class="language-plaintext highlighter-rouge">$\Z$</code>, or polynomials with rational coefficients <code class="language-plaintext highlighter-rouge">$\Q[x]$</code>, for example.</p> <p>Having chosen our coefficients <code class="language-plaintext highlighter-rouge">$K$</code>, a linear space <code class="language-plaintext highlighter-rouge">$V$</code> <em>over</em> <code class="language-plaintext highlighter-rouge">$K$</code> is another set of objects that can be added and subtracted (and including a special value <code class="language-plaintext highlighter-rouge">$0$</code>)<sup id="fnref:ab-group" role="doc-noteref"><a href="#fn:ab-group" class="footnote" rel="footnote">2</a></sup>, along with a <em>scaling operation</em>, which takes a coefficient <code class="language-plaintext highlighter-rouge">$c \in K$</code> and one of our objects <code class="language-plaintext highlighter-rouge">$v \in V$</code> and produces a new <code class="language-plaintext highlighter-rouge">$cv \in V$</code>.</p> <p>The important part of the scaling operation is that it’s compatible with addition: if we have <code class="language-plaintext highlighter-rouge">$a, b \in K$</code> and <code class="language-plaintext highlighter-rouge">$v, w \in V$</code>, we require that</p> <p>```latex render:gather* a (v + w) = av + aw <br/> (a + b) v = av + bv</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
This is what makes a linear space "linear": you can write equations that look
like first-degree polynomials (e.g. `$ax + b$`), and which can be _manipulated
like first-degree polynomials_.

These polynomials are called linear because their graph looks like a line.
There's no multiplication, so we can't have `$x^2$`, but we do have
multiplication by a coefficient. This is what makes linear algebra is "linear".

Some examples: `$n$`-tuples of elements drawn from any field are a linear space
over that field, by componentwise addition and scalar multiplication; e.g.,
`$R^3$`. Setting `$n = 1$` shows that every field is a linear space over itself.

Polynomials in one variable over some field, `$K[x]$`, are also a linear space,
since polynomials can be added together and scaled by a any value in `$K$`
(since lone coefficients are degree zero polynomials). Real-valued functions
also form a linear space over `$\R$` in a similar way.

### Linear Transformations

A linear map is a function `$f: V \to W$` between two linear spaces `$V$` and
`$W$` over `$K$` which "respects" the linear structure in a particular way. That
is, for any `$c\in K$` and `$v, w \in V$`,

```latex render:gather*
f(v + w) = f(v) + f(w) \\
f(cv) = c \cdot f(v)
</code></pre></div></div> <p>We call this type of relationship (respecting addition and scaling) “linearity”. One way to think of this relationship is that <code class="language-plaintext highlighter-rouge">$f$</code> is kind of like a different kind of coefficient, in that it distributes over addition, which commutes with the “ordinary” coefficients from <code class="language-plaintext highlighter-rouge">$K$</code>. However, applying <code class="language-plaintext highlighter-rouge">$f$</code> produces a value from <code class="language-plaintext highlighter-rouge">$W$</code> rather than <code class="language-plaintext highlighter-rouge">$V$</code>.</p> <p>Another way to think of it is that if we have a linear polynomial like <code class="language-plaintext highlighter-rouge">$p(x) = ax + b$</code> in <code class="language-plaintext highlighter-rouge">$x$</code>, then <code class="language-plaintext highlighter-rouge">$f(p(x)) = p(f(x))$</code>. We say that <code class="language-plaintext highlighter-rouge">$f$</code> <em>commutes</em> with all linear polynomials.</p> <p>The most obvious sort of linear map is scaling. Given any coefficient <code class="language-plaintext highlighter-rouge">$c \in K$</code>, it defines a “scaling map”:</p> <p>```latex render:gather* \mu_c: V \to V <br/> v \mapsto cv</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
It's trivial to check this is a linear map, by plugging it into the above
equations: it's linear because scaling is distributive and commutative.

Linear maps are the essential thing we study in linear algebra, since they
describe all the different kinds of relationships between linear spaces.

Some linear maps are complicated. For example, a function from `$\R^2 \to \R^2$`
that rotates the plane by some angle `$\theta$` is linear, as are operations
that stretch or shear the plane. However, they can't "bend" or "fold" the plane:
they are all fairly rigid motions. In the linear space `$\Q[x]$` of rational
polynomials, multiplication by _any_ polynomial, such as `$x$` or `$x^2 - 1$`,
is a linear map. The notion of "linear map" depends heavily on the space we're
in.

Unfortunately, linear maps as they are quite opaque, and do not lend themselves
well to calculation. However, we can build an explicit representation using a
_linear basis_.

## Linear Basis

For any linear space, we can construct a relatively small of elements such that
any element of the space can be expressed as some linear function of these
elements.

Explicitly, for any `$V$`, we can construct a sequence[^indices] `$e_i$` such
that for any `$v \in V$`, we can find `$c_i \in K$` such that

[^indices]:
    Throughout `$i$`, `$j$`, and `$k$` are indices in some unspecified but
    ordered indexing set, usually `$\{1, 2, ..., n\}$`. I will not bother giving
    this index set a name.

```latex render:
v = \sum_i c_i e_i.
</code></pre></div></div> <p>Such a set <code class="language-plaintext highlighter-rouge">$e_i$</code> is called a <em>basis</em> if it is linearly independent: no one <code class="language-plaintext highlighter-rouge">$e_i$</code> can be expressed as a linear function of the rest. The <em>dimension</em> of <code class="language-plaintext highlighter-rouge">$V$</code>, denoted <code class="language-plaintext highlighter-rouge">$\dim V$</code>, is the number of elements in any choice of basis. This value does not depend on the choice of basis<sup id="fnref:invariant-dim" role="doc-noteref"><a href="#fn:invariant-dim" class="footnote" rel="footnote">3</a></sup>.</p> <p>Constructing a basis for any <code class="language-plaintext highlighter-rouge">$V$</code> is easy: we can do this recursively. First, pick a random element <code class="language-plaintext highlighter-rouge">$e_1$</code> of <code class="language-plaintext highlighter-rouge">$V$</code>, and define a new linear space <code class="language-plaintext highlighter-rouge">$V/e_1$</code> where we have identified all elements that differ by a factor of <code class="language-plaintext highlighter-rouge">$e_1$</code> as equal (i.e., if <code class="language-plaintext highlighter-rouge">$v - w = ce_1$</code>, we treat <code class="language-plaintext highlighter-rouge">$v$</code> and <code class="language-plaintext highlighter-rouge">$w$</code> as equal in <code class="language-plaintext highlighter-rouge">$V/e_1$</code>).</p> <p>Then, a basis for <code class="language-plaintext highlighter-rouge">$V$</code> is a basis of <code class="language-plaintext highlighter-rouge">$V/e_1$</code> with <code class="language-plaintext highlighter-rouge">$e_1$</code> added. The construction of <code class="language-plaintext highlighter-rouge">$V/e_1$</code> is essentially “collapsing” the dimension <code class="language-plaintext highlighter-rouge">$e_1$</code> “points” in, giving us a new space where we’ve “deleted” all of the elements that have a nonzero <code class="language-plaintext highlighter-rouge">$e_1$</code> component.</p> <p>However, this only works when the dimension is finite; more complex methods must be used for infinite-dimensional spaces. For example, the polynomials <code class="language-plaintext highlighter-rouge">$\Q[x]$</code> are an infinite-dimensional space, with basis elements <code class="language-plaintext highlighter-rouge">$\\{1, x, x^2, x^3, ...\\}$</code>. In general, for any linear space <code class="language-plaintext highlighter-rouge">$V$</code>, it <em>is</em> always possible to arbitrarily choose a basis, although it may be infinite<sup id="fnref:rq" role="doc-noteref"><a href="#fn:rq" class="footnote" rel="footnote">4</a></sup>.</p> <p>Bases are useful because they give us a concrete representation of any element of <code class="language-plaintext highlighter-rouge">$V$</code>. Given a fixed basis <code class="language-plaintext highlighter-rouge">$e_i$</code>, we can represent any <code class="language-plaintext highlighter-rouge">$w = \sum_i c_i e_i$</code> by the coefficients <code class="language-plaintext highlighter-rouge">$c_i$</code> themselves. For a finite-dimensional <code class="language-plaintext highlighter-rouge">$V$</code>, this brings us back <em>column vectors</em>: <code class="language-plaintext highlighter-rouge">$(\dim V)$</code>-tuples of coefficients from <code class="language-plaintext highlighter-rouge">$K$</code> that are added and scaled componentwise.</p> <p>```latex render: \Mat{c_0 \ c_1 \ \vdots \ c_n} \,\underset{\text{given } e_i}{:=}\, \sum_i c_i e_i</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
The `$i$`th basis element is represented as the vector whose entries are all
`$0$` except for the `$i$`th one, which is `$1$`. E.g.,

```latex render:
\Mat{1 \\ 0 \\ \vdots \\ 0} \,\underset{\text{given } e_i}{=}\, e_1, \,\,\,
\Mat{0 \\ 1 \\ \vdots \\ 0} \,\underset{\text{given } e_i}{=}\, e_2, \,\,\,
...
</code></pre></div></div> <p>It is important to recall that the choice of basis is <em>arbitrary</em>. From the mathematical perspective, any basis is just as good as any other, although some may be more computationally convenient.</p> <p>Over <code class="language-plaintext highlighter-rouge">$\R^2$</code>, <code class="language-plaintext highlighter-rouge">$(1, 0)$</code> and <code class="language-plaintext highlighter-rouge">$(0, 1)$</code> are sometimes called the “standard basis”, but <code class="language-plaintext highlighter-rouge">$(1, 2)$</code> and <code class="language-plaintext highlighter-rouge">$(3, -4)$</code> are also a basis for this space. One easy mistake to make, particularly when working over the tuple space <code class="language-plaintext highlighter-rouge">$K^n$</code>, is to confuse the actual elements of the linear space with the coefficient vectors that represent them. Working with abstract linear spaces eliminates this source of confusion.</p> <h3 id="representing-linear-transformations"><a href="#representing-linear-transformations">Representing Linear Transformations</a></h3> <p>Working with finite-dimensional linear spaces <code class="language-plaintext highlighter-rouge">$V$</code> and <code class="language-plaintext highlighter-rouge">$W$</code>, let’s choose bases <code class="language-plaintext highlighter-rouge">$e_i$</code> and <code class="language-plaintext highlighter-rouge">$d_j$</code> for them, and let’s consider a linear map <code class="language-plaintext highlighter-rouge">$f: V \to W$</code>.</p> <p>The powerful thing about bases is that we can more compactly express the information content of <code class="language-plaintext highlighter-rouge">$f$</code>. Given any <code class="language-plaintext highlighter-rouge">$v \in V$</code>, we can decompose it into a linear function of the basis (for some coefficients), so we can write</p> <p>```latex render: f(v) = f\left(\sum_i c_i e_i\right) = \sum_i f(c_i e_i) = \sum_i c_i \cdot f(e_i)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
In other words, to specify `$f$`, we _only_ need to specify what it does to each
of the `$\dim V$` basis elements. But what's more, because `$W$` also has a
basis, we can write

```latex render:
f(e_i) = \sum_j A_{ij} d_j
</code></pre></div></div> <p>Putting these two formulas together, we have an explicit closed form for <code class="language-plaintext highlighter-rouge">$f(v)$</code>, given the coefficients <code class="language-plaintext highlighter-rouge">$A_{ij}$</code> of <code class="language-plaintext highlighter-rouge">$f$</code>, and the coefficients <code class="language-plaintext highlighter-rouge">$c_i$</code> of <code class="language-plaintext highlighter-rouge">$v$</code>:</p> <p>```latex render: f(v) = \sum_{i,j} c_i A_{ij} d_j</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Alternatively, we can express `$v$` and `$f(v)$` as column vectors, and `$f$` as
the `$A$` matrix with entires `$A_{ij}$`. The entries of the resulting column
vector are given by the above explicit formula for `$f(v)$`, fixing the value of
`$j$` in each entry.

```latex render:
\underbrace{\Mat{
  A_{0,0} &amp; A_{1,0} &amp; \cdots &amp; A_{n,0} \\
  A_{1,0} &amp; A_{1,1} &amp; \cdots &amp; A_{n,1} \\
  \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
  A_{0,m} &amp; A_{1,m} &amp; \cdots &amp; A_{n,m}
}}_A \,
\underbrace{\Mat{c_0 \\ c_1 \\ \vdots \\ c_n}}_v =
\underbrace{\Mat{
  \sum_i c_i A_{i,0} \\
  \sum_i c_i A_{i,1} \\
  \vdots \\
  \sum_i c_i A_{i,m}
}}_{Av}
</code></pre></div></div> <p>(Remember, this is all dependent on the choices of bases <code class="language-plaintext highlighter-rouge">$e_i$</code> and <code class="language-plaintext highlighter-rouge">$d_j$</code>!)</p> <p>Behold, we have derived the matrix-vector multiplication formula: the <code class="language-plaintext highlighter-rouge">$j$</code>th entry of the result is the dot product of the vector and the <code class="language-plaintext highlighter-rouge">$j$</code>th row of the matrix.</p> <p>But it is crucial to keep in mind that we had to choose bases <code class="language-plaintext highlighter-rouge">$e_i$</code> and <code class="language-plaintext highlighter-rouge">$d_j$</code> to be entitled to write down a matrix for <code class="language-plaintext highlighter-rouge">$f$</code>. The values of the coefficients depend on the choice of basis.</p> <p>If your linear space happens to be <code class="language-plaintext highlighter-rouge">$\R^n$</code>, there is an “obvious” choice of basis, but not every linear space over <code class="language-plaintext highlighter-rouge">$\R$</code> is <code class="language-plaintext highlighter-rouge">$\R^n$</code>! Importantly, the actual linear algebra <em>does not</em> change depending on the basis<sup id="fnref:similar" role="doc-noteref"><a href="#fn:similar" class="footnote" rel="footnote">5</a></sup>.</p> <h2 id="matrix-multiplication"><a href="#matrix-multiplication">Matrix Multiplication</a></h2> <p>So, where does matrix multiplication come from? An <code class="language-plaintext highlighter-rouge">$n \times m$</code><sup id="fnref:rc" role="doc-noteref"><a href="#fn:rc" class="footnote" rel="footnote">6</a></sup> matrix <code class="language-plaintext highlighter-rouge">$A$</code> <em>represents</em> some linear map <code class="language-plaintext highlighter-rouge">$f: V \to W$</code>, where <code class="language-plaintext highlighter-rouge">$\dim V = n$</code>, <code class="language-plaintext highlighter-rouge">$\dim W = m$</code>, and appropriate choices of basis (<code class="language-plaintext highlighter-rouge">$e_i$</code>, <code class="language-plaintext highlighter-rouge">$d_j$</code>) have been made.</p> <p>Keeping in mind that linear maps are supreme over matrices, suppose we have a third linear space <code class="language-plaintext highlighter-rouge">$U$</code>, and a map <code class="language-plaintext highlighter-rouge">$g: U \to V$</code>, and let <code class="language-plaintext highlighter-rouge">$\ell = \dim U$</code>. Choosing a basis <code class="language-plaintext highlighter-rouge">$h_k$</code> for <code class="language-plaintext highlighter-rouge">$U$</code>, we can represent <code class="language-plaintext highlighter-rouge">$g$</code> as a matrix <code class="language-plaintext highlighter-rouge">$B$</code> of dimension <code class="language-plaintext highlighter-rouge">$\ell \times n$</code>.</p> <p>Then, we’d like for the matrix product <code class="language-plaintext highlighter-rouge">$AB$</code> to be the same matrix we’d get from representing the composite map <code class="language-plaintext highlighter-rouge">$fg: U \to W$</code> as a matrix, using the aforementioned choices of bases for <code class="language-plaintext highlighter-rouge">$U$</code> and <code class="language-plaintext highlighter-rouge">$W$</code> (the basis choice for <code class="language-plaintext highlighter-rouge">$V$</code> should “cancel out”).</p> <p>Recall our formula for <code class="language-plaintext highlighter-rouge">$f(v)$</code> in terms of its matrix coefficients <code class="language-plaintext highlighter-rouge">$A_{ij}$</code> and the coefficients of the input <code class="language-plaintext highlighter-rouge">$v$</code>, which we call <code class="language-plaintext highlighter-rouge">$c_i$</code>. We can produce a similar formula for <code class="language-plaintext highlighter-rouge">$g(u)$</code>, giving it matrix coefficients <code class="language-plaintext highlighter-rouge">$B_{ki}$</code>, and coefficients <code class="language-plaintext highlighter-rouge">$b_k$</code> for <code class="language-plaintext highlighter-rouge">$u$</code>. (I appologize for the number of indices and coefficients here.)</p> <p>```latex render:align* f(v) &amp;= \sum_{i,j} c_i A_{ij} d_j <br/> g(u) &amp;= \sum_{k,i} b_k B_{ki} e_i</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
If we write `$f(g(u))$`, then `$c_i$` is the coefficient `$e_i$` is multiplied
by; i.e., we fix `$i$`, and drop it from the summation:
`$c_i = \sum_k b_k B_{ki}$`.

Substituting that into the above formula, we now have something like the
following.

```latex render:align*
f(g(u)) &amp;= \sum_{i,j} \sum_{k} b_k B_{ki} A_{ij} d_j \\
f(g(u)) &amp;= \sum_{k,j} b_k \left(\sum_{i} A_{ij} B_{ki} \right) d_j &amp;(\star)
</code></pre></div></div> <p>In <code class="language-plaintext highlighter-rouge">$(\star)$</code>, we’ve rearranged things so that the sum in parenthesis is the <code class="language-plaintext highlighter-rouge">$(k,j)$</code>th matrix coefficient of the composite <code class="language-plaintext highlighter-rouge">$fg$</code>. Because we wanted <code class="language-plaintext highlighter-rouge">$AB$</code> to represent <code class="language-plaintext highlighter-rouge">$fg$</code>, it must be an <code class="language-plaintext highlighter-rouge">$\ell \times m$</code> matrix whose entries are</p> <p>```latex render: (AB)<em>{kj} = \sum</em>{i} A_{ij} B_{ki}</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
_This_ is matrix multiplication. It arises naturally out of composition of
linear maps. In this way, the matrix multiplication formula is not a definition,
but a _theorem_ of linear algebra!

&gt; #### Theorem (Matrix Multiplication)
&gt;
&gt; Given an `$n \times m$` matrix `$A$` and an `$\ell \times n$` matrix `$B$`,
&gt; both with coefficients in `$K$`, then `$AB$` is an `$\ell \times m$` matrix
&gt; with entires
&gt;
&gt; ```latex render:
&gt; (AB)_{kj} = \sum_{i} A_{ij} B_{ki}
&gt; ```

If the matrix dimension is read as `$n \to m$` instead of `$n \times m$`, the
shape requirements are more obvious: two matrices `$A$` and `$B$` can be
multiplied together only when they represent a pair of maps `$V \to W$` and
`$U \to V$`.

### Other Consequences, and Conclusion

The identity matrix is an `$n \times n$` matrix:

```latex render:
I_n = \Mat{
1 \\
&amp; 1 \\
&amp;&amp; \ddots \\
&amp;&amp;&amp; 1
}
</code></pre></div></div> <p>We want it to be such that for any appropriately-sized matrices <code class="language-plaintext highlighter-rouge">$A$</code> and <code class="language-plaintext highlighter-rouge">$B$</code>, it has <code class="language-plaintext highlighter-rouge">$AI_n = A$</code> and <code class="language-plaintext highlighter-rouge">$I_n B = B$</code>. Lifted up to linear maps, this means that <code class="language-plaintext highlighter-rouge">$I_n$</code> should represent the identity map <code class="language-plaintext highlighter-rouge">$V \to V$</code>, when <code class="language-plaintext highlighter-rouge">$\dim V = n$</code>. This map sends each basis element <code class="language-plaintext highlighter-rouge">$e_i$</code> to itself, so the columns of <code class="language-plaintext highlighter-rouge">$I_n$</code> should be the basis vectors, in order:</p> <p>```latex render: \Mat{1 \ 0 \ \vdots \ 0} \Mat{0 \ 1 \ \vdots \ 0} \cdots \Mat{0 \ 0 \ \vdots \ 1}</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
If we shuffle the columns, we'll get a _permutation matrix_, which shuffles the
coefficients of a column vector. For example, consider this matrix.

```latex render:
\Mat{
  0 &amp; 1 &amp; 0 \\
  1 &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; 1
}
</code></pre></div></div> <p>This is similar to the identity, but we’ve swapped the first two columns. Thus, it will swap the first two coefficients of any column vector.</p> <p>Matrices may seem unintuitive when they’re introduced as a subject of study. Every student encountering matrices for the same time may ask “If they add componentwise, why don’t they multiply componentwise too?”</p> <p>However, approaching matrices as a computational and representational tool shows that the convoluted-looking matrix multiplication formula is a direct consequence of linearity.</p> <p><code class="language-plaintext highlighter-rouge">latex render:gather* f(v + w) = f(v) + f(w) \\ f(cv) = c \cdot f(v) </code></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:vs-in-generality" role="doc-endnote"> <p>In actual modern mathematics, the objects I describe are still called vector spaces, which I think generates unnecessary confusion in this case. “Linear space” is a bit more on the nose for what I’m going for. <a href="#fnref:vs-in-generality" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:ab-group" role="doc-endnote"> <p>This type of structure (just the addition part) is also called an “abelian group”. <a href="#fnref:ab-group" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:invariant-dim" role="doc-endnote"> <p>This is sometimes called the <a href="https://en.wikipedia.org/wiki/Dimension_theorem_for_vector_spaces"><em>dimension theorem</em></a>, which is somewhat tedious to prove. <a href="#fnref:invariant-dim" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:rq" role="doc-endnote"> <p>An example of a messy infinite-dimensional basis is <code class="language-plaintext highlighter-rouge">$\R$</code> considered as linear space over <code class="language-plaintext highlighter-rouge">$\Q$</code> (in general, every field is a linear space over its subfields). The basis for this space essentially has to be “<code class="language-plaintext highlighter-rouge">$1$</code>, and all irrational numbers” except if we include e.g. <code class="language-plaintext highlighter-rouge">$e$</code> and <code class="language-plaintext highlighter-rouge">$\pi$</code> we can’t include <code class="language-plaintext highlighter-rouge">$e + \frac{1}{2}\pi$</code>, which is a <code class="language-plaintext highlighter-rouge">$\Q$</code>-linear combination of <code class="language-plaintext highlighter-rouge">$e$</code> and <code class="language-plaintext highlighter-rouge">$\pi$</code>.</p> <p>On the other hand, <code class="language-plaintext highlighter-rouge">$\C$</code> is two-dimensional over <code class="language-plaintext highlighter-rouge">$\R$</code>, with basis <code class="language-plaintext highlighter-rouge">$\\{1, i\\}$</code>.</p> <p>Incidentally, this idea of “view a field <code class="language-plaintext highlighter-rouge">$K$</code> as a linear space over its subfield <code class="language-plaintext highlighter-rouge">$F$</code>” is such a useful concept that it is called the “degree of the field extension <code class="language-plaintext highlighter-rouge">$K/F$</code>”, and given the symbol <code class="language-plaintext highlighter-rouge">$[K : F]$</code>.</p> <p>This, <code class="language-plaintext highlighter-rouge">$[\R : \Q] = \infty$</code> and <code class="language-plaintext highlighter-rouge">$[\C : \R] = 2$</code>. <a href="#fnref:rq" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:similar" role="doc-endnote"> <p>You may recall from linear algebra class that two matrices <code class="language-plaintext highlighter-rouge">$A$</code> and <code class="language-plaintext highlighter-rouge">$B$</code> of the same shape are <em>similar</em> if there are two appropriately-sized square matrices <code class="language-plaintext highlighter-rouge">$S$</code> and <code class="language-plaintext highlighter-rouge">$R$</code> such that <code class="language-plaintext highlighter-rouge">$SAR = B$</code>. These matrices <code class="language-plaintext highlighter-rouge">$S$</code> and <code class="language-plaintext highlighter-rouge">$R$</code> represent a <em>change of basis</em>, and indicate that the linear maps <code class="language-plaintext highlighter-rouge">$A, B: V \to W$</code> these matrices come from do “the same thing” to elements of <code class="language-plaintext highlighter-rouge">$V$</code>.</p> <p>Over an algebraically closed field like <code class="language-plaintext highlighter-rouge">$\C$</code> (i.e. all polynomials have solutions), there is an even stronger way to capture the information content of a linear map via <a href="https://en.wikipedia.org/wiki/Jordan_normal_form"><em>Jordan canonicalization</em></a>, which takes any square matrix <code class="language-plaintext highlighter-rouge">$A$</code> and produces an almost-diagonal square matrix that only depends on the eigenvalues of <code class="language-plaintext highlighter-rouge">$A$</code>, which is the same for similar matrices, and thus basis-independent. <a href="#fnref:similar" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:rc" role="doc-endnote"> <p>Here, as always, matrix dimensions are given in RC (row-column) order. You can think of this as being “input dimension” to “output dimension”. <a href="#fnref:rc" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div> </div> <div class="related post-footer"> <h2>Related Posts</h2> <ul class="related-posts"> <li> <span class="post-meta">2023-11-27</span> / <h6 style="display:inline"><a href="/2023/11/27/simd-base64/">Designing a SIMD Algorithm from Scratch</a></h6> <li> <span class="post-meta">2023-08-09</span> / <h6 style="display:inline"><a href="/2023/08/09/yarns/">I Wrote A String Type </a></h6> <li> <span class="post-meta">2023-08-01</span> / <h6 style="display:inline"><a href="/2023/08/01/llvm-ir/">A Gentle Introduction to LLVM IR </a></h6> </ul> </div> <div class="minimap"> <div class="minimap-size"></div> <div class="minimap-controller"></div> <div class="minimap-content"> <div class="content container"> <div class="post-title"> <span class="post-meta"> 2023-09-29 • 2009 words • 11 minutes <br class="show-if-mobile"/> <span class="hide-if-mobile">•</span> <a href="https://mcyoung.xyz/tags.html#math">#math</a> </span> <h1><a href="/2023/09/29/what-is-a-matrix/"> What is a Matrix? A Miserable Pile of Coefficients! </a></h1> </div> <div class="post"> <p>Linear algebra is undoubtedly the most useful field in all of algebra. It finds applications in all kinds of science and engineering, like quantum mechanics, graphics programming, and machine learning. It is the “most well-behaved” algebraic theory, in that other abstract algebra topics often try to approximate linear algebra, when possible.</p> <p>For many students, linear algebra means vectors and matrices and determinants, and complex formulas for computing them. Matrices, in particular, come equipped with a fairly complicated, and <em>a fortiori</em> convoluted, multiplication operation.</p> <p>This is not the only way to teach linear algebra, of course. Matrices and their multiplication appear complicated, but actually are a natural and compact way to represent a particular type of <em>function</em>, i.e., a linear map (or linear transformation).</p> <p>This article is a short introduction to viewing linear algebra from the perspective of abstract algebra, from which matrices arise as a computational tool, rather than an object of study in and of themselves. I do assume some degree of familiarity with the idea of a matrix.</p> <h2 id="linear-spaces"><a href="#linear-spaces">Linear Spaces</a></h2> <p>Most linear algebra courses open with a description of vectors in Euclidean space: <code class="language-plaintext highlighter-rouge">$\R^n$</code>. Vectors there are defined as tuples of real numbers that can be added, multiplied, and scaled. Two vectors can be combined into a number through the dot product. Vectors come equipped with a notion of magnitude and direction.</p> <p>However, this highly geometric picture can be counterproductive, since it is hard to apply geometric intuition directly to higher dimensions. It also obscures how this connects to working over a different number system, like the complex numbers.</p> <p>Instead, I’d like to open with the concept of a <em>linear space</em>, which is somewhat more abstract than a vector space<sup id="fnref:vs-in-generality" role="doc-noteref"><a href="#fn:vs-in-generality" class="footnote" rel="footnote">1</a></sup>.</p> <p>First, we will need a notion of a “coefficient”, which is essentially something that you can do arithmetic with. We will draw coefficients from a designated <em>ground field</em> <code class="language-plaintext highlighter-rouge">$K$</code>. A field is a setting for doing arithmetic: a set of objects that can be added, subtracted, and multiplied, and divided in the “usual fashion” along with special <code class="language-plaintext highlighter-rouge">$0$</code> and <code class="language-plaintext highlighter-rouge">$1$</code> values. E.g. <code class="language-plaintext highlighter-rouge">$a + 0 = a$</code>, <code class="language-plaintext highlighter-rouge">$1a = a$</code>, <code class="language-plaintext highlighter-rouge">$a(b + c) = ab + ac$</code>, and so on.</p> <p>Not only are the real numbers <code class="language-plaintext highlighter-rouge">$\R$</code> a field, but so are the complex numbers <code class="language-plaintext highlighter-rouge">$\C$</code>, and the rational numbers <code class="language-plaintext highlighter-rouge">$\Q$</code>. If we drop the “division” requirement, we can also include the integers <code class="language-plaintext highlighter-rouge">$\Z$</code>, or polynomials with rational coefficients <code class="language-plaintext highlighter-rouge">$\Q[x]$</code>, for example.</p> <p>Having chosen our coefficients <code class="language-plaintext highlighter-rouge">$K$</code>, a linear space <code class="language-plaintext highlighter-rouge">$V$</code> <em>over</em> <code class="language-plaintext highlighter-rouge">$K$</code> is another set of objects that can be added and subtracted (and including a special value <code class="language-plaintext highlighter-rouge">$0$</code>)<sup id="fnref:ab-group" role="doc-noteref"><a href="#fn:ab-group" class="footnote" rel="footnote">2</a></sup>, along with a <em>scaling operation</em>, which takes a coefficient <code class="language-plaintext highlighter-rouge">$c \in K$</code> and one of our objects <code class="language-plaintext highlighter-rouge">$v \in V$</code> and produces a new <code class="language-plaintext highlighter-rouge">$cv \in V$</code>.</p> <p>The important part of the scaling operation is that it’s compatible with addition: if we have <code class="language-plaintext highlighter-rouge">$a, b \in K$</code> and <code class="language-plaintext highlighter-rouge">$v, w \in V$</code>, we require that</p> <p>```latex render:gather* a (v + w) = av + aw <br/> (a + b) v = av + bv</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
This is what makes a linear space "linear": you can write equations that look
like first-degree polynomials (e.g. `$ax + b$`), and which can be _manipulated
like first-degree polynomials_.

These polynomials are called linear because their graph looks like a line.
There's no multiplication, so we can't have `$x^2$`, but we do have
multiplication by a coefficient. This is what makes linear algebra is "linear".

Some examples: `$n$`-tuples of elements drawn from any field are a linear space
over that field, by componentwise addition and scalar multiplication; e.g.,
`$R^3$`. Setting `$n = 1$` shows that every field is a linear space over itself.

Polynomials in one variable over some field, `$K[x]$`, are also a linear space,
since polynomials can be added together and scaled by a any value in `$K$`
(since lone coefficients are degree zero polynomials). Real-valued functions
also form a linear space over `$\R$` in a similar way.

### Linear Transformations

A linear map is a function `$f: V \to W$` between two linear spaces `$V$` and
`$W$` over `$K$` which "respects" the linear structure in a particular way. That
is, for any `$c\in K$` and `$v, w \in V$`,

```latex render:gather*
f(v + w) = f(v) + f(w) \\
f(cv) = c \cdot f(v)
</code></pre></div></div> <p>We call this type of relationship (respecting addition and scaling) “linearity”. One way to think of this relationship is that <code class="language-plaintext highlighter-rouge">$f$</code> is kind of like a different kind of coefficient, in that it distributes over addition, which commutes with the “ordinary” coefficients from <code class="language-plaintext highlighter-rouge">$K$</code>. However, applying <code class="language-plaintext highlighter-rouge">$f$</code> produces a value from <code class="language-plaintext highlighter-rouge">$W$</code> rather than <code class="language-plaintext highlighter-rouge">$V$</code>.</p> <p>Another way to think of it is that if we have a linear polynomial like <code class="language-plaintext highlighter-rouge">$p(x) = ax + b$</code> in <code class="language-plaintext highlighter-rouge">$x$</code>, then <code class="language-plaintext highlighter-rouge">$f(p(x)) = p(f(x))$</code>. We say that <code class="language-plaintext highlighter-rouge">$f$</code> <em>commutes</em> with all linear polynomials.</p> <p>The most obvious sort of linear map is scaling. Given any coefficient <code class="language-plaintext highlighter-rouge">$c \in K$</code>, it defines a “scaling map”:</p> <p>```latex render:gather* \mu_c: V \to V <br/> v \mapsto cv</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
It's trivial to check this is a linear map, by plugging it into the above
equations: it's linear because scaling is distributive and commutative.

Linear maps are the essential thing we study in linear algebra, since they
describe all the different kinds of relationships between linear spaces.

Some linear maps are complicated. For example, a function from `$\R^2 \to \R^2$`
that rotates the plane by some angle `$\theta$` is linear, as are operations
that stretch or shear the plane. However, they can't "bend" or "fold" the plane:
they are all fairly rigid motions. In the linear space `$\Q[x]$` of rational
polynomials, multiplication by _any_ polynomial, such as `$x$` or `$x^2 - 1$`,
is a linear map. The notion of "linear map" depends heavily on the space we're
in.

Unfortunately, linear maps as they are quite opaque, and do not lend themselves
well to calculation. However, we can build an explicit representation using a
_linear basis_.

## Linear Basis

For any linear space, we can construct a relatively small of elements such that
any element of the space can be expressed as some linear function of these
elements.

Explicitly, for any `$V$`, we can construct a sequence[^indices] `$e_i$` such
that for any `$v \in V$`, we can find `$c_i \in K$` such that

[^indices]:
    Throughout `$i$`, `$j$`, and `$k$` are indices in some unspecified but
    ordered indexing set, usually `$\{1, 2, ..., n\}$`. I will not bother giving
    this index set a name.

```latex render:
v = \sum_i c_i e_i.
</code></pre></div></div> <p>Such a set <code class="language-plaintext highlighter-rouge">$e_i$</code> is called a <em>basis</em> if it is linearly independent: no one <code class="language-plaintext highlighter-rouge">$e_i$</code> can be expressed as a linear function of the rest. The <em>dimension</em> of <code class="language-plaintext highlighter-rouge">$V$</code>, denoted <code class="language-plaintext highlighter-rouge">$\dim V$</code>, is the number of elements in any choice of basis. This value does not depend on the choice of basis<sup id="fnref:invariant-dim" role="doc-noteref"><a href="#fn:invariant-dim" class="footnote" rel="footnote">3</a></sup>.</p> <p>Constructing a basis for any <code class="language-plaintext highlighter-rouge">$V$</code> is easy: we can do this recursively. First, pick a random element <code class="language-plaintext highlighter-rouge">$e_1$</code> of <code class="language-plaintext highlighter-rouge">$V$</code>, and define a new linear space <code class="language-plaintext highlighter-rouge">$V/e_1$</code> where we have identified all elements that differ by a factor of <code class="language-plaintext highlighter-rouge">$e_1$</code> as equal (i.e., if <code class="language-plaintext highlighter-rouge">$v - w = ce_1$</code>, we treat <code class="language-plaintext highlighter-rouge">$v$</code> and <code class="language-plaintext highlighter-rouge">$w$</code> as equal in <code class="language-plaintext highlighter-rouge">$V/e_1$</code>).</p> <p>Then, a basis for <code class="language-plaintext highlighter-rouge">$V$</code> is a basis of <code class="language-plaintext highlighter-rouge">$V/e_1$</code> with <code class="language-plaintext highlighter-rouge">$e_1$</code> added. The construction of <code class="language-plaintext highlighter-rouge">$V/e_1$</code> is essentially “collapsing” the dimension <code class="language-plaintext highlighter-rouge">$e_1$</code> “points” in, giving us a new space where we’ve “deleted” all of the elements that have a nonzero <code class="language-plaintext highlighter-rouge">$e_1$</code> component.</p> <p>However, this only works when the dimension is finite; more complex methods must be used for infinite-dimensional spaces. For example, the polynomials <code class="language-plaintext highlighter-rouge">$\Q[x]$</code> are an infinite-dimensional space, with basis elements <code class="language-plaintext highlighter-rouge">$\\{1, x, x^2, x^3, ...\\}$</code>. In general, for any linear space <code class="language-plaintext highlighter-rouge">$V$</code>, it <em>is</em> always possible to arbitrarily choose a basis, although it may be infinite<sup id="fnref:rq" role="doc-noteref"><a href="#fn:rq" class="footnote" rel="footnote">4</a></sup>.</p> <p>Bases are useful because they give us a concrete representation of any element of <code class="language-plaintext highlighter-rouge">$V$</code>. Given a fixed basis <code class="language-plaintext highlighter-rouge">$e_i$</code>, we can represent any <code class="language-plaintext highlighter-rouge">$w = \sum_i c_i e_i$</code> by the coefficients <code class="language-plaintext highlighter-rouge">$c_i$</code> themselves. For a finite-dimensional <code class="language-plaintext highlighter-rouge">$V$</code>, this brings us back <em>column vectors</em>: <code class="language-plaintext highlighter-rouge">$(\dim V)$</code>-tuples of coefficients from <code class="language-plaintext highlighter-rouge">$K$</code> that are added and scaled componentwise.</p> <p>```latex render: \Mat{c_0 \ c_1 \ \vdots \ c_n} \,\underset{\text{given } e_i}{:=}\, \sum_i c_i e_i</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
The `$i$`th basis element is represented as the vector whose entries are all
`$0$` except for the `$i$`th one, which is `$1$`. E.g.,

```latex render:
\Mat{1 \\ 0 \\ \vdots \\ 0} \,\underset{\text{given } e_i}{=}\, e_1, \,\,\,
\Mat{0 \\ 1 \\ \vdots \\ 0} \,\underset{\text{given } e_i}{=}\, e_2, \,\,\,
...
</code></pre></div></div> <p>It is important to recall that the choice of basis is <em>arbitrary</em>. From the mathematical perspective, any basis is just as good as any other, although some may be more computationally convenient.</p> <p>Over <code class="language-plaintext highlighter-rouge">$\R^2$</code>, <code class="language-plaintext highlighter-rouge">$(1, 0)$</code> and <code class="language-plaintext highlighter-rouge">$(0, 1)$</code> are sometimes called the “standard basis”, but <code class="language-plaintext highlighter-rouge">$(1, 2)$</code> and <code class="language-plaintext highlighter-rouge">$(3, -4)$</code> are also a basis for this space. One easy mistake to make, particularly when working over the tuple space <code class="language-plaintext highlighter-rouge">$K^n$</code>, is to confuse the actual elements of the linear space with the coefficient vectors that represent them. Working with abstract linear spaces eliminates this source of confusion.</p> <h3 id="representing-linear-transformations"><a href="#representing-linear-transformations">Representing Linear Transformations</a></h3> <p>Working with finite-dimensional linear spaces <code class="language-plaintext highlighter-rouge">$V$</code> and <code class="language-plaintext highlighter-rouge">$W$</code>, let’s choose bases <code class="language-plaintext highlighter-rouge">$e_i$</code> and <code class="language-plaintext highlighter-rouge">$d_j$</code> for them, and let’s consider a linear map <code class="language-plaintext highlighter-rouge">$f: V \to W$</code>.</p> <p>The powerful thing about bases is that we can more compactly express the information content of <code class="language-plaintext highlighter-rouge">$f$</code>. Given any <code class="language-plaintext highlighter-rouge">$v \in V$</code>, we can decompose it into a linear function of the basis (for some coefficients), so we can write</p> <p>```latex render: f(v) = f\left(\sum_i c_i e_i\right) = \sum_i f(c_i e_i) = \sum_i c_i \cdot f(e_i)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
In other words, to specify `$f$`, we _only_ need to specify what it does to each
of the `$\dim V$` basis elements. But what's more, because `$W$` also has a
basis, we can write

```latex render:
f(e_i) = \sum_j A_{ij} d_j
</code></pre></div></div> <p>Putting these two formulas together, we have an explicit closed form for <code class="language-plaintext highlighter-rouge">$f(v)$</code>, given the coefficients <code class="language-plaintext highlighter-rouge">$A_{ij}$</code> of <code class="language-plaintext highlighter-rouge">$f$</code>, and the coefficients <code class="language-plaintext highlighter-rouge">$c_i$</code> of <code class="language-plaintext highlighter-rouge">$v$</code>:</p> <p>```latex render: f(v) = \sum_{i,j} c_i A_{ij} d_j</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Alternatively, we can express `$v$` and `$f(v)$` as column vectors, and `$f$` as
the `$A$` matrix with entires `$A_{ij}$`. The entries of the resulting column
vector are given by the above explicit formula for `$f(v)$`, fixing the value of
`$j$` in each entry.

```latex render:
\underbrace{\Mat{
  A_{0,0} &amp; A_{1,0} &amp; \cdots &amp; A_{n,0} \\
  A_{1,0} &amp; A_{1,1} &amp; \cdots &amp; A_{n,1} \\
  \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
  A_{0,m} &amp; A_{1,m} &amp; \cdots &amp; A_{n,m}
}}_A \,
\underbrace{\Mat{c_0 \\ c_1 \\ \vdots \\ c_n}}_v =
\underbrace{\Mat{
  \sum_i c_i A_{i,0} \\
  \sum_i c_i A_{i,1} \\
  \vdots \\
  \sum_i c_i A_{i,m}
}}_{Av}
</code></pre></div></div> <p>(Remember, this is all dependent on the choices of bases <code class="language-plaintext highlighter-rouge">$e_i$</code> and <code class="language-plaintext highlighter-rouge">$d_j$</code>!)</p> <p>Behold, we have derived the matrix-vector multiplication formula: the <code class="language-plaintext highlighter-rouge">$j$</code>th entry of the result is the dot product of the vector and the <code class="language-plaintext highlighter-rouge">$j$</code>th row of the matrix.</p> <p>But it is crucial to keep in mind that we had to choose bases <code class="language-plaintext highlighter-rouge">$e_i$</code> and <code class="language-plaintext highlighter-rouge">$d_j$</code> to be entitled to write down a matrix for <code class="language-plaintext highlighter-rouge">$f$</code>. The values of the coefficients depend on the choice of basis.</p> <p>If your linear space happens to be <code class="language-plaintext highlighter-rouge">$\R^n$</code>, there is an “obvious” choice of basis, but not every linear space over <code class="language-plaintext highlighter-rouge">$\R$</code> is <code class="language-plaintext highlighter-rouge">$\R^n$</code>! Importantly, the actual linear algebra <em>does not</em> change depending on the basis<sup id="fnref:similar" role="doc-noteref"><a href="#fn:similar" class="footnote" rel="footnote">5</a></sup>.</p> <h2 id="matrix-multiplication"><a href="#matrix-multiplication">Matrix Multiplication</a></h2> <p>So, where does matrix multiplication come from? An <code class="language-plaintext highlighter-rouge">$n \times m$</code><sup id="fnref:rc" role="doc-noteref"><a href="#fn:rc" class="footnote" rel="footnote">6</a></sup> matrix <code class="language-plaintext highlighter-rouge">$A$</code> <em>represents</em> some linear map <code class="language-plaintext highlighter-rouge">$f: V \to W$</code>, where <code class="language-plaintext highlighter-rouge">$\dim V = n$</code>, <code class="language-plaintext highlighter-rouge">$\dim W = m$</code>, and appropriate choices of basis (<code class="language-plaintext highlighter-rouge">$e_i$</code>, <code class="language-plaintext highlighter-rouge">$d_j$</code>) have been made.</p> <p>Keeping in mind that linear maps are supreme over matrices, suppose we have a third linear space <code class="language-plaintext highlighter-rouge">$U$</code>, and a map <code class="language-plaintext highlighter-rouge">$g: U \to V$</code>, and let <code class="language-plaintext highlighter-rouge">$\ell = \dim U$</code>. Choosing a basis <code class="language-plaintext highlighter-rouge">$h_k$</code> for <code class="language-plaintext highlighter-rouge">$U$</code>, we can represent <code class="language-plaintext highlighter-rouge">$g$</code> as a matrix <code class="language-plaintext highlighter-rouge">$B$</code> of dimension <code class="language-plaintext highlighter-rouge">$\ell \times n$</code>.</p> <p>Then, we’d like for the matrix product <code class="language-plaintext highlighter-rouge">$AB$</code> to be the same matrix we’d get from representing the composite map <code class="language-plaintext highlighter-rouge">$fg: U \to W$</code> as a matrix, using the aforementioned choices of bases for <code class="language-plaintext highlighter-rouge">$U$</code> and <code class="language-plaintext highlighter-rouge">$W$</code> (the basis choice for <code class="language-plaintext highlighter-rouge">$V$</code> should “cancel out”).</p> <p>Recall our formula for <code class="language-plaintext highlighter-rouge">$f(v)$</code> in terms of its matrix coefficients <code class="language-plaintext highlighter-rouge">$A_{ij}$</code> and the coefficients of the input <code class="language-plaintext highlighter-rouge">$v$</code>, which we call <code class="language-plaintext highlighter-rouge">$c_i$</code>. We can produce a similar formula for <code class="language-plaintext highlighter-rouge">$g(u)$</code>, giving it matrix coefficients <code class="language-plaintext highlighter-rouge">$B_{ki}$</code>, and coefficients <code class="language-plaintext highlighter-rouge">$b_k$</code> for <code class="language-plaintext highlighter-rouge">$u$</code>. (I appologize for the number of indices and coefficients here.)</p> <p>```latex render:align* f(v) &amp;= \sum_{i,j} c_i A_{ij} d_j <br/> g(u) &amp;= \sum_{k,i} b_k B_{ki} e_i</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
If we write `$f(g(u))$`, then `$c_i$` is the coefficient `$e_i$` is multiplied
by; i.e., we fix `$i$`, and drop it from the summation:
`$c_i = \sum_k b_k B_{ki}$`.

Substituting that into the above formula, we now have something like the
following.

```latex render:align*
f(g(u)) &amp;= \sum_{i,j} \sum_{k} b_k B_{ki} A_{ij} d_j \\
f(g(u)) &amp;= \sum_{k,j} b_k \left(\sum_{i} A_{ij} B_{ki} \right) d_j &amp;(\star)
</code></pre></div></div> <p>In <code class="language-plaintext highlighter-rouge">$(\star)$</code>, we’ve rearranged things so that the sum in parenthesis is the <code class="language-plaintext highlighter-rouge">$(k,j)$</code>th matrix coefficient of the composite <code class="language-plaintext highlighter-rouge">$fg$</code>. Because we wanted <code class="language-plaintext highlighter-rouge">$AB$</code> to represent <code class="language-plaintext highlighter-rouge">$fg$</code>, it must be an <code class="language-plaintext highlighter-rouge">$\ell \times m$</code> matrix whose entries are</p> <p>```latex render: (AB)<em>{kj} = \sum</em>{i} A_{ij} B_{ki}</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
_This_ is matrix multiplication. It arises naturally out of composition of
linear maps. In this way, the matrix multiplication formula is not a definition,
but a _theorem_ of linear algebra!

&gt; #### Theorem (Matrix Multiplication)
&gt;
&gt; Given an `$n \times m$` matrix `$A$` and an `$\ell \times n$` matrix `$B$`,
&gt; both with coefficients in `$K$`, then `$AB$` is an `$\ell \times m$` matrix
&gt; with entires
&gt;
&gt; ```latex render:
&gt; (AB)_{kj} = \sum_{i} A_{ij} B_{ki}
&gt; ```

If the matrix dimension is read as `$n \to m$` instead of `$n \times m$`, the
shape requirements are more obvious: two matrices `$A$` and `$B$` can be
multiplied together only when they represent a pair of maps `$V \to W$` and
`$U \to V$`.

### Other Consequences, and Conclusion

The identity matrix is an `$n \times n$` matrix:

```latex render:
I_n = \Mat{
1 \\
&amp; 1 \\
&amp;&amp; \ddots \\
&amp;&amp;&amp; 1
}
</code></pre></div></div> <p>We want it to be such that for any appropriately-sized matrices <code class="language-plaintext highlighter-rouge">$A$</code> and <code class="language-plaintext highlighter-rouge">$B$</code>, it has <code class="language-plaintext highlighter-rouge">$AI_n = A$</code> and <code class="language-plaintext highlighter-rouge">$I_n B = B$</code>. Lifted up to linear maps, this means that <code class="language-plaintext highlighter-rouge">$I_n$</code> should represent the identity map <code class="language-plaintext highlighter-rouge">$V \to V$</code>, when <code class="language-plaintext highlighter-rouge">$\dim V = n$</code>. This map sends each basis element <code class="language-plaintext highlighter-rouge">$e_i$</code> to itself, so the columns of <code class="language-plaintext highlighter-rouge">$I_n$</code> should be the basis vectors, in order:</p> <p>```latex render: \Mat{1 \ 0 \ \vdots \ 0} \Mat{0 \ 1 \ \vdots \ 0} \cdots \Mat{0 \ 0 \ \vdots \ 1}</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
If we shuffle the columns, we'll get a _permutation matrix_, which shuffles the
coefficients of a column vector. For example, consider this matrix.

```latex render:
\Mat{
  0 &amp; 1 &amp; 0 \\
  1 &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; 1
}
</code></pre></div></div> <p>This is similar to the identity, but we’ve swapped the first two columns. Thus, it will swap the first two coefficients of any column vector.</p> <p>Matrices may seem unintuitive when they’re introduced as a subject of study. Every student encountering matrices for the same time may ask “If they add componentwise, why don’t they multiply componentwise too?”</p> <p>However, approaching matrices as a computational and representational tool shows that the convoluted-looking matrix multiplication formula is a direct consequence of linearity.</p> <p><code class="language-plaintext highlighter-rouge">latex render:gather* f(v + w) = f(v) + f(w) \\ f(cv) = c \cdot f(v) </code></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:vs-in-generality" role="doc-endnote"> <p>In actual modern mathematics, the objects I describe are still called vector spaces, which I think generates unnecessary confusion in this case. “Linear space” is a bit more on the nose for what I’m going for. <a href="#fnref:vs-in-generality" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:ab-group" role="doc-endnote"> <p>This type of structure (just the addition part) is also called an “abelian group”. <a href="#fnref:ab-group" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:invariant-dim" role="doc-endnote"> <p>This is sometimes called the <a href="https://en.wikipedia.org/wiki/Dimension_theorem_for_vector_spaces"><em>dimension theorem</em></a>, which is somewhat tedious to prove. <a href="#fnref:invariant-dim" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:rq" role="doc-endnote"> <p>An example of a messy infinite-dimensional basis is <code class="language-plaintext highlighter-rouge">$\R$</code> considered as linear space over <code class="language-plaintext highlighter-rouge">$\Q$</code> (in general, every field is a linear space over its subfields). The basis for this space essentially has to be “<code class="language-plaintext highlighter-rouge">$1$</code>, and all irrational numbers” except if we include e.g. <code class="language-plaintext highlighter-rouge">$e$</code> and <code class="language-plaintext highlighter-rouge">$\pi$</code> we can’t include <code class="language-plaintext highlighter-rouge">$e + \frac{1}{2}\pi$</code>, which is a <code class="language-plaintext highlighter-rouge">$\Q$</code>-linear combination of <code class="language-plaintext highlighter-rouge">$e$</code> and <code class="language-plaintext highlighter-rouge">$\pi$</code>.</p> <p>On the other hand, <code class="language-plaintext highlighter-rouge">$\C$</code> is two-dimensional over <code class="language-plaintext highlighter-rouge">$\R$</code>, with basis <code class="language-plaintext highlighter-rouge">$\\{1, i\\}$</code>.</p> <p>Incidentally, this idea of “view a field <code class="language-plaintext highlighter-rouge">$K$</code> as a linear space over its subfield <code class="language-plaintext highlighter-rouge">$F$</code>” is such a useful concept that it is called the “degree of the field extension <code class="language-plaintext highlighter-rouge">$K/F$</code>”, and given the symbol <code class="language-plaintext highlighter-rouge">$[K : F]$</code>.</p> <p>This, <code class="language-plaintext highlighter-rouge">$[\R : \Q] = \infty$</code> and <code class="language-plaintext highlighter-rouge">$[\C : \R] = 2$</code>. <a href="#fnref:rq" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:similar" role="doc-endnote"> <p>You may recall from linear algebra class that two matrices <code class="language-plaintext highlighter-rouge">$A$</code> and <code class="language-plaintext highlighter-rouge">$B$</code> of the same shape are <em>similar</em> if there are two appropriately-sized square matrices <code class="language-plaintext highlighter-rouge">$S$</code> and <code class="language-plaintext highlighter-rouge">$R$</code> such that <code class="language-plaintext highlighter-rouge">$SAR = B$</code>. These matrices <code class="language-plaintext highlighter-rouge">$S$</code> and <code class="language-plaintext highlighter-rouge">$R$</code> represent a <em>change of basis</em>, and indicate that the linear maps <code class="language-plaintext highlighter-rouge">$A, B: V \to W$</code> these matrices come from do “the same thing” to elements of <code class="language-plaintext highlighter-rouge">$V$</code>.</p> <p>Over an algebraically closed field like <code class="language-plaintext highlighter-rouge">$\C$</code> (i.e. all polynomials have solutions), there is an even stronger way to capture the information content of a linear map via <a href="https://en.wikipedia.org/wiki/Jordan_normal_form"><em>Jordan canonicalization</em></a>, which takes any square matrix <code class="language-plaintext highlighter-rouge">$A$</code> and produces an almost-diagonal square matrix that only depends on the eigenvalues of <code class="language-plaintext highlighter-rouge">$A$</code>, which is the same for similar matrices, and thus basis-independent. <a href="#fnref:similar" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:rc" role="doc-endnote"> <p>Here, as always, matrix dimensions are given in RC (row-column) order. You can think of this as being “input dimension” to “output dimension”. <a href="#fnref:rc" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div> </div> <div class="related post-footer"> <h2>Related Posts</h2> <ul class="related-posts"> <li> <span class="post-meta">2023-11-27</span> / <h6 style="display:inline"><a href="/2023/11/27/simd-base64/">Designing a SIMD Algorithm from Scratch</a></h6> <li> <span class="post-meta">2023-08-09</span> / <h6 style="display:inline"><a href="/2023/08/09/yarns/">I Wrote A String Type </a></h6> <li> <span class="post-meta">2023-08-01</span> / <h6 style="display:inline"><a href="/2023/08/01/llvm-ir/">A Gentle Introduction to LLVM IR </a></h6> </ul> </div> </div> </div> </div></div> <div class="sidebar show-if-mobile footer"> <div class="container sidebar-sticky"> <a href="https://creativecommons.org/licenses/by-sa/4.0/legalcode">CC BY-SA</a> • <a href="https://varz.mcyoung.xyz/">Site Analytics</a> <br> &copy; 2024 Miguel Young de la Sota </div> </div> </body> </html>